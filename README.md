# PromptPapers


![](https://img.shields.io/github/last-commit/thunlp/PromptPapers?color=blue) ![](https://img.shields.io/badge/PaperNumber-12-brightgreen) ![](https://img.shields.io/badge/PRs-Welcome-red) 


Must-read papers on prompt-based tuning for pre-trained language models. The paper list is mainly mantained by Ning Ding and Shengding Hu.

## Introduction

This is a paper list about *prompt-based tuning* for large-scale pre-trained language models.

## Papers

1. **Language Models are Few-shot Learners.** *Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei.*  Preprint.    [[pdf](https://arxiv.org/abs/2005.14165)], [[website](https://openai.com/blog/gpt-3-apps/)]  **(GPT-3)**
2. **Language Models as Knowledge Bases?**  *Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel.* EMNLP 2019.  [[pdf](https://arxiv.org/pdf/1909.01066.pdf)], [[project](https://github.com/facebookresearch/LAMA)] **(LAMA)**
3. **The Power of Scale for Parameter-EfÔ¨Åcient Prompt Tuning.** *Brian Lester, Rami Al-Rfou, Noah Constant*. Preprint. [[pdf](https://arxiv.org/pdf/2104.08691.pdf)], [[implementation](https://github.com/kipgparker/soft-prompt-tuning)]
4. **Factual Probing Is [MASK]: Learning vs. Learning to Recall.** *Zexuan Zhong, Dan Friedman, Danqi Chen.* NAACL 2021.  [[pdf](https://arxiv.org/pdf/2104.05240.pdf)], [[project](https://github.com/princeton-nlp/OptiPrompt)] 
5. **How Can We Know What Language Models Know?** *Zhengbao Jiang, Frank F. Xu, Jun Araki, Graham Neubig*. TACL 2020. [[pdf](https://arxiv.org/pdf/1911.12543.pdf)], [[project](https://github.com/jzbjyb/LPAQA)]
6. **Autoprompt: Eliciting knowledge from language models with automatically generated prompts.** *Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, Sameer Singh.* Preprint. [[pdf](https://arxiv.org/pdf/2010.15980.pdf)], [[website](https://ucinlp.github.io/autoprompt/)] **(AutoPrompt)**
7. **Calibrate Before Use: Improving Few-Shot Performance of Language Models.** *Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh.*  [[pdf](https://arxiv.org/pdf/2102.09690.pdf)], [[project](https://github.com/tonyzhaozh/few-shot-learning)]
8. **Making Pre-trained Language Models Better Few-shot Learners.** *Tianyu Gao, Adam Fisch, Danqi Chen.* ACL 2021. [[pdf](https://arxiv.org/pdf/2012.15723.pdf)][[project](https://github.com/princeton-nlp/LM-BFF)] **(LM-BFF)**
9. **GPT understands, too.** *Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang*. Preprint. [[pdf](https://arxiv.org/pdf/2103.10385.pdf)], [[project](https://github.com/THUDM/P-tuning)] **(P-tuning)**
10. **AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction.** *Xiang Chen, Xin Xie, Ningyu Zhang, Jiahuan Yan, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, Huajun Chen*. Preprint. [[pdf](https://arxiv.org/pdf/2104.07650.pdf)]
11. **PTR: Prompt Tuning with Rules for Text Classification.**  *Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, Maosong Sun.* Preprint. [[pdf](https://arxiv.org/pdf/2105.11259.pdf)] **(PTR)**
12. **Prefix-tuning: Optimizing continuous prompts for generation**. *Xiang Lisa Li, Percy Liang.* Preprint. [[pdf](https://arxiv.org/pdf/2101.00190.pdf)], [[project](https://github.com/XiangLi1999/PrefixTuning)]
